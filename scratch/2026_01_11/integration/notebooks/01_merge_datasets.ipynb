{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 01 - Merge Datasets\n\nThis notebook merges multiple h5ad files into a single AnnData object for integration.\n\n## Inputs\n- List of h5ad file paths\n- Batch key name\n- Optional downsampling configuration\n\n## Outputs\n- `merged.h5ad` - Full combined dataset (always saved)\n- `merged_random_{n}.h5ad` - Random downsampled versions (optional)\n- `merged_celltype_{n}pertype.h5ad` - Cell type aware downsampled versions (optional)\n- `*.summary.txt` - Summary reports for each output"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/opt/miniconda3/envs/scverse-lite-py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# For nice progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sc.settings.verbosity = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Edit the parameters below or load from a config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option 1: Load from config file\n# config_path = \"../config/sc-islet_integration_test.yaml\"\n# with open(config_path) as f:\n#     config = yaml.safe_load(f)\n\n# Option 2: Define parameters directly\nconfig = {\n    \"input\": {\n        \"files\": [\n            {\"path\": \"/cellar/users/aklie/data/datasets/igvf_sc-islet_10X-Multiome/results/4_integration/2025_11_30/rna/integrate/round_1/annotation/annotated.h5ad\", \"name\": \"igvf_sc-islet_10X-Multiome\"},\n            {\"path\": \"/cellar/users/aklie/data/datasets/Augsornworawat2023_sc-islet_10X-Multiome/results/4_cell_annotation/rna/integrate/round_1/annotation/annotated.h5ad\", \"name\": \"Augsornworawat2023_sc-islet_10X-Multiome\"},\n        ],\n        \"batch_key\": \"dataset\",\n    },\n    \"output\": {\n        \"dir\": \"/cellar/users/aklie/projects/igvf/beta_cell_networks/scratch/2026_01_11/integration/results\",\n    },\n    # Downsampling configuration (optional)\n    # The full merged dataset is ALWAYS saved\n    # Downsampled versions are saved as additional files\n    \"downsampling\": {\n        \"seed\": 42,\n        # Random downsampling - list of target cell counts (or single value)\n        # Each value creates: merged_random_{n}.h5ad\n        \"random\": {\n            \"n_cells\": [],  # e.g., [10000, 25000, 50000] or 50000\n        },\n        # Cell type aware downsampling\n        # Each value creates: merged_celltype_{n}pertype.h5ad\n        \"celltype_aware\": {\n            \"celltype_column\": \"cell_type\",\n            \"n_cells_per_type\": [],  # e.g., [500, 1000] or 1000\n            \"keep_unshared\": False,\n            \"min_cells_per_type\": 10,\n        },\n    },\n}"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files: 2\n",
      "Batch key: dataset\n",
      "Output directory: /cellar/users/aklie/projects/igvf/beta_cell_networks/scratch/2026_01_11/integration/results\n"
     ]
    }
   ],
   "source": [
    "# Extract config values\n",
    "input_files = config[\"input\"][\"files\"]\n",
    "batch_key = config[\"input\"][\"batch_key\"]\n",
    "output_dir = Path(config[\"output\"][\"dir\"])\n",
    "\n",
    "# Create output directory\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input files: {len(input_files)}\")\n",
    "print(f\"Batch key: {batch_key}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading igvf_sc-islet_10X-Multiome from /cellar/users/aklie/data/datasets/igvf_sc-islet_10X-Multiome/results/4_integration/2025_11_30/rna/integrate/round_1/annotation/annotated.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets:  50%|█████     | 1/2 [02:44<02:44, 164.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (328420, 36601)\n",
      "  Obs columns: ['gex_barcode_cellranger', 'atac_barcode_cellranger', 'is_cell_cellranger', 'excluded_reason_cellranger', 'gex_raw_reads_cellranger', 'gex_mapped_reads_cellranger', 'gex_conf_intergenic_reads_cellranger', 'gex_conf_exonic_reads_cellranger', 'gex_conf_intronic_reads_cellranger', 'gex_conf_exonic_unique_reads_cellranger']...\n",
      "\n",
      "Loading Augsornworawat2023_sc-islet_10X-Multiome from /cellar/users/aklie/data/datasets/Augsornworawat2023_sc-islet_10X-Multiome/results/4_cell_annotation/rna/integrate/round_1/annotation/annotated.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets: 100%|██████████| 2/2 [04:23<00:00, 131.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (44060, 36601)\n",
      "  Obs columns: ['gex_barcode_cellranger', 'atac_barcode_cellranger', 'is_cell_cellranger', 'excluded_reason_cellranger', 'gex_raw_reads_cellranger', 'gex_mapped_reads_cellranger', 'gex_conf_intergenic_reads_cellranger', 'gex_conf_exonic_reads_cellranger', 'gex_conf_intronic_reads_cellranger', 'gex_conf_exonic_unique_reads_cellranger']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "adatas = {}\n",
    "\n",
    "for file_info in tqdm(input_files, desc=\"Loading datasets\"):\n",
    "    path = file_info[\"path\"]\n",
    "    name = file_info[\"name\"]\n",
    "    \n",
    "    print(f\"\\nLoading {name} from {path}\")\n",
    "    adata = sc.read_h5ad(path)\n",
    "    \n",
    "    # Add batch label\n",
    "    adata.obs[batch_key] = name\n",
    "    \n",
    "    # Store\n",
    "    adatas[name] = adata\n",
    "    \n",
    "    print(f\"  Shape: {adata.shape}\")\n",
    "    print(f\"  Obs columns: {list(adata.obs.columns)[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect datasets before merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>n_cells</th>\n",
       "      <th>n_genes</th>\n",
       "      <th>has_raw</th>\n",
       "      <th>layers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>igvf_sc-islet_10X-Multiome</td>\n",
       "      <td>328420</td>\n",
       "      <td>36601</td>\n",
       "      <td>True</td>\n",
       "      <td>[counts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Augsornworawat2023_sc-islet_10X-Multiome</td>\n",
       "      <td>44060</td>\n",
       "      <td>36601</td>\n",
       "      <td>True</td>\n",
       "      <td>[counts]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    dataset  n_cells  n_genes  has_raw  \\\n",
       "0                igvf_sc-islet_10X-Multiome   328420    36601     True   \n",
       "1  Augsornworawat2023_sc-islet_10X-Multiome    44060    36601     True   \n",
       "\n",
       "     layers  \n",
       "0  [counts]  \n",
       "1  [counts]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary table\n",
    "summary = []\n",
    "for name, adata in adatas.items():\n",
    "    summary.append({\n",
    "        \"dataset\": name,\n",
    "        \"n_cells\": adata.n_obs,\n",
    "        \"n_genes\": adata.n_vars,\n",
    "        \"has_raw\": adata.raw is not None,\n",
    "        \"layers\": list(adata.layers.keys()) if adata.layers else [],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common genes: 36601\n",
      "Total unique genes: 36601\n",
      "Overlap: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Check gene overlap\n",
    "gene_sets = [set(adata.var_names) for adata in adatas.values()]\n",
    "common_genes = set.intersection(*gene_sets)\n",
    "all_genes = set.union(*gene_sets)\n",
    "\n",
    "print(f\"Common genes: {len(common_genes)}\")\n",
    "print(f\"Total unique genes: {len(all_genes)}\")\n",
    "print(f\"Overlap: {len(common_genes) / len(all_genes) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating with join=inner...\n",
      "\n",
      "Merged shape: (372480, 36601)\n",
      "Cells per batch:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "igvf_sc-islet_10X-Multiome                  328420\n",
       "Augsornworawat2023_sc-islet_10X-Multiome     44060\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use outer join to keep all genes, or inner to keep only common\n",
    "join_type = \"inner\"  # or \"outer\"\n",
    "\n",
    "print(f\"Concatenating with join={join_type}...\")\n",
    "adata_merged = ad.concat(\n",
    "    list(adatas.values()),\n",
    "    join=join_type,\n",
    "    label=batch_key,\n",
    "    keys=list(adatas.keys()),\n",
    "    index_unique=\"_\",  # Add suffix to duplicate indices\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged shape: {adata_merged.shape}\")\n",
    "print(f\"Cells per batch:\")\n",
    "display(adata_merged.obs[batch_key].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify merge and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged obs columns:\n",
      "['gex_barcode_cellranger', 'atac_barcode_cellranger', 'is_cell_cellranger', 'excluded_reason_cellranger', 'gex_raw_reads_cellranger', 'gex_mapped_reads_cellranger', 'gex_conf_intergenic_reads_cellranger', 'gex_conf_exonic_reads_cellranger', 'gex_conf_intronic_reads_cellranger', 'gex_conf_exonic_unique_reads_cellranger', 'gex_conf_exonic_antisense_reads_cellranger', 'gex_conf_exonic_dup_reads_cellranger', 'gex_exonic_umis_cellranger', 'gex_conf_intronic_unique_reads_cellranger', 'gex_conf_intronic_antisense_reads_cellranger', 'gex_conf_intronic_dup_reads_cellranger', 'gex_intronic_umis_cellranger', 'gex_conf_txomic_unique_reads_cellranger', 'gex_umis_count_cellranger', 'gex_genes_count_cellranger', 'atac_raw_reads_cellranger', 'atac_unmapped_reads_cellranger', 'atac_lowmapq_cellranger', 'atac_dup_reads_cellranger', 'atac_chimeric_reads_cellranger', 'atac_mitochondrial_reads_cellranger', 'atac_fragments_cellranger', 'atac_TSS_fragments_cellranger', 'atac_peak_region_fragments_cellranger', 'atac_peak_region_cutsites_cellranger', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'outlier', 'mt_outlier', 'ribo_outlier', 'pre_soupx_leiden_1', 'post_soupx_leiden_1', 'n_genes_by_counts_post_soupx', 'total_counts_post_soupx', 'pct_counts_in_top_20_genes_post_soupx', 'total_counts_mt_post_soupx', 'pct_counts_mt_post_soupx', 'total_counts_ribo_post_soupx', 'pct_counts_ribo_post_soupx', 'log1p_total_counts_post_soupx', 'scrublet_doublet_score', 'scrublet_predicted_doublet', 'scDblFinder_doublet_score', 'scDblFinder_doublet_class', 'scDblFinder_predicted_doublet', 'cellranger_predicted_doublet', 'doublet_filter', 'pre_doublet_filter_leiden_1', 'leiden_1', 'sample', 'harmony_round_1_leiden_0.2', 'harmony_round_1_leiden_0.5', 'harmony_round_1_leiden_0.8', 'harmony_round_1_leiden_1.0', 'cell_type', 'dataset']\n"
     ]
    }
   ],
   "source": [
    "# Ensure batch_key is categorical\n",
    "adata_merged.obs[batch_key] = adata_merged.obs[batch_key].astype(\"category\")\n",
    "\n",
    "# Remove any duplicate var columns that may have been created\n",
    "# (keep first occurrence)\n",
    "if adata_merged.var.columns.duplicated().any():\n",
    "    adata_merged.var = adata_merged.var.loc[:, ~adata_merged.var.columns.duplicated()]\n",
    "\n",
    "# Summary of obs columns\n",
    "print(\"Merged obs columns:\")\n",
    "print(adata_merged.obs.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "No missing values in obs.\n"
     ]
    }
   ],
   "source": [
    "# Quick QC check\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing = adata_merged.obs.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if len(missing) > 0:\n",
    "    display(missing)\n",
    "else:\n",
    "    print(\"No missing values in obs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Save full merged dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save full merged dataset (always)\noutput_path = output_dir / \"merged.h5ad\"\nprint(f\"Saving full dataset to {output_path}...\")\nadata_merged.write_h5ad(output_path)\n\n# Save summary\nsummary_path = output_dir / \"merged.summary.txt\"\nwith open(summary_path, \"w\") as f:\n    f.write(\"=== Merge Summary ===\\n\\n\")\n    f.write(f\"Total cells: {adata_merged.n_obs}\\n\")\n    f.write(f\"Total genes: {adata_merged.n_vars}\\n\")\n    f.write(f\"Batch key: {batch_key}\\n\\n\")\n    f.write(\"Cells per batch:\\n\")\n    for batch, count in adata_merged.obs[batch_key].value_counts().items():\n        f.write(f\"  {batch}: {count}\\n\")\n\nprint(f\"Full dataset saved: {output_path}\")\nprint(f\"Summary saved: {summary_path}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Downsampling (Optional)\n\nTwo downsampling methods are available for creating smaller test datasets:\n\n1. **Random**: Simple random downsampling to a target cell count\n2. **Cell type aware**: Match cell types between datasets, optionally remove non-shared types\n\nDownsampled versions are saved as additional files:\n- Random: `merged_random_{n_cells}.h5ad`\n- Cell type aware: `merged_celltype_{n_per_type}pertype.h5ad`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def ensure_list(value):\n    \"\"\"Convert a single value to a list if needed.\"\"\"\n    if value is None:\n        return []\n    if isinstance(value, (list, tuple)):\n        return list(value)\n    return [value]\n\n\ndef downsample_random(adatas, n_cells, per_dataset=None, seed=42):\n    \"\"\"Randomly downsample datasets to a target cell count.\"\"\"\n    np.random.seed(seed)\n    \n    if per_dataset is None:\n        total = sum(adata.n_obs for adata in adatas.values())\n        per_dataset = {\n            name: int(n_cells * adata.n_obs / total)\n            for name, adata in adatas.items()\n        }\n    \n    downsampled = {}\n    for name, adata in adatas.items():\n        target = per_dataset.get(name, adata.n_obs)\n        target = min(target, adata.n_obs)\n        \n        if target < adata.n_obs:\n            indices = np.random.choice(adata.n_obs, size=target, replace=False)\n            indices = np.sort(indices)\n            downsampled[name] = adata[indices].copy()\n            print(f\"  {name}: {adata.n_obs} -> {target} cells\")\n        else:\n            downsampled[name] = adata.copy()\n            print(f\"  {name}: {adata.n_obs} cells (no downsampling needed)\")\n    \n    return downsampled\n\n\ndef downsample_celltype_aware(adatas, celltype_column, n_cells_per_type=1000,\n                               keep_unshared=False, min_cells_per_type=10, seed=42):\n    \"\"\"Downsample datasets with cell type awareness.\"\"\"\n    np.random.seed(seed)\n    \n    # Find cell types in each dataset\n    celltype_sets = {\n        name: set(adata.obs[celltype_column].dropna().unique())\n        for name, adata in adatas.items()\n    }\n    \n    shared_celltypes = set.intersection(*celltype_sets.values())\n    all_celltypes = set.union(*celltype_sets.values())\n    \n    print(f\"\\n  Cell types per dataset:\")\n    for name, ct_set in celltype_sets.items():\n        print(f\"    {name}: {len(ct_set)} types\")\n    print(f\"  Shared cell types: {len(shared_celltypes)}\")\n    print(f\"  Total unique cell types: {len(all_celltypes)}\")\n    \n    if not keep_unshared:\n        print(f\"  Keeping only shared cell types: {sorted(shared_celltypes)}\")\n        celltypes_to_keep = shared_celltypes\n    else:\n        celltypes_to_keep = all_celltypes\n    \n    downsampled = {}\n    for name, adata in adatas.items():\n        indices_to_keep = []\n        \n        for celltype in celltypes_to_keep:\n            mask = adata.obs[celltype_column] == celltype\n            ct_indices = np.where(mask)[0]\n            \n            if len(ct_indices) < min_cells_per_type:\n                continue\n            \n            if len(ct_indices) > n_cells_per_type:\n                sampled = np.random.choice(ct_indices, size=n_cells_per_type, replace=False)\n            else:\n                sampled = ct_indices\n            \n            indices_to_keep.extend(sampled)\n        \n        indices_to_keep = np.sort(indices_to_keep)\n        downsampled[name] = adata[indices_to_keep].copy()\n        \n        orig_types = adata.obs[celltype_column].nunique()\n        new_types = downsampled[name].obs[celltype_column].nunique()\n        print(f\"  {name}: {adata.n_obs} -> {len(indices_to_keep)} cells \"\n              f\"({orig_types} -> {new_types} cell types)\")\n    \n    return downsampled\n\n\ndef merge_adatas(adatas, batch_key, join_type=\"inner\"):\n    \"\"\"Merge a dictionary of AnnData objects.\"\"\"\n    adata_merged = ad.concat(\n        list(adatas.values()),\n        join=join_type,\n        label=batch_key,\n        keys=list(adatas.keys()),\n        index_unique=\"_\",\n    )\n    adata_merged.obs[batch_key] = adata_merged.obs[batch_key].astype(\"category\")\n    return adata_merged\n\n\nprint(\"Downsampling functions defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Apply downsampling and save based on config\nds_config = config.get(\"downsampling\", {})\nseed = ds_config.get(\"seed\", 42)\nsaved_files = [str(output_path)]\n\n# Random downsampling\nrandom_config = ds_config.get(\"random\", {})\nrandom_n_cells = ensure_list(random_config.get(\"n_cells\", []))\n\nfor n_cells in random_n_cells:\n    print(f\"\\n{'='*50}\")\n    print(f\"Random downsampling: {n_cells} total cells\")\n    print('='*50)\n    adatas_ds = downsample_random(adatas, n_cells=n_cells, seed=seed)\n    adata_ds = merge_adatas(adatas_ds, batch_key)\n    \n    ds_path = output_dir / f\"merged_random_{n_cells}.h5ad\"\n    print(f\"\\nSaving to {ds_path}...\")\n    adata_ds.write_h5ad(ds_path)\n    saved_files.append(str(ds_path))\n    print(f\"Shape: {adata_ds.shape}\")\n    \n    del adatas_ds, adata_ds\n\n# Cell type aware downsampling\nct_config = ds_config.get(\"celltype_aware\", {})\nct_n_per_type = ensure_list(ct_config.get(\"n_cells_per_type\", []))\nct_column = ct_config.get(\"celltype_column\", \"cell_type\")\nkeep_unshared = ct_config.get(\"keep_unshared\", False)\nmin_cells = ct_config.get(\"min_cells_per_type\", 10)\n\nfor n_per_type in ct_n_per_type:\n    print(f\"\\n{'='*50}\")\n    print(f\"Cell type aware downsampling: {n_per_type} cells per type\")\n    print('='*50)\n    adatas_ds = downsample_celltype_aware(\n        adatas,\n        celltype_column=ct_column,\n        n_cells_per_type=n_per_type,\n        keep_unshared=keep_unshared,\n        min_cells_per_type=min_cells,\n        seed=seed,\n    )\n    adata_ds = merge_adatas(adatas_ds, batch_key)\n    \n    ds_path = output_dir / f\"merged_celltype_{n_per_type}pertype.h5ad\"\n    print(f\"\\nSaving to {ds_path}...\")\n    adata_ds.write_h5ad(ds_path)\n    saved_files.append(str(ds_path))\n    print(f\"Shape: {adata_ds.shape}\")\n    \n    del adatas_ds, adata_ds\n\n# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"All saved files:\")\nprint(\"=\"*50)\nfor f in saved_files:\n    print(f\"  {f}\")\nprint(\"\\nDone!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Next steps\n\nThe merged dataset(s) are now ready for integration. Proceed to one of:\n\n- `02_harmony_integration.ipynb` - Harmony (via R/rpy2)\n- `03_seurat_integration.ipynb` - Seurat methods (CCA, RPCA, FastMNN)  \n- `04_python_integration.ipynb` - scVI and Scanorama (pure Python)\n- `04_seurat_integrationR.ipynb` - Pure R Seurat integration\n\nFor testing, use the downsampled files (e.g., `merged_celltype_1000pertype.h5ad`).",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 scverse-lite",
   "language": "python",
   "name": "scverse-lite-py11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}